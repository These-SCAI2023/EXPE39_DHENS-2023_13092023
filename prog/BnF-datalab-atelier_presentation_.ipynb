{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <td style=\"text-align:center\">\n",
    "   \n",
    "   <h2>Evaluations unitaires de chaque composante de la chaîne de traitement Épiméthée </h2>\n",
    "        <h3> Atelier BnF DataLab </h3>\n",
    "        <h3>\"Manipuler des données en SHS : R, Python et les carnets interactifs\" </h3>\n",
    "    <h3><br>Caroline Koudoro-Parfait(1,2,3),</h3>\n",
    "    <h3><br>Marceau Hernandez(4),</h3>\n",
    "        <h3>caroline.parfait@sorbonne-universite.fr</h3>\n",
    "        <h3>marceau.hernandez@sorbonne-universite.fr</h3>\n",
    "        1) OBTIC, Sorbonne Université, Paris, France<br>\n",
    "(2) STIH, Sorbonne Université, Paris, France<br>\n",
    "(3) SCAI, Sorbonne Center for Artificial Intelligence, Paris, France<br>\n",
    "(4) Ceres, Sorbonne Université, Paris, France<br>\n",
    "    </td>     \n",
    "</table>\n",
    "<table>\n",
    "     <td style=\"text-align:center\"><img src=\"../DATA/images/github.png\" width=\"15\" height=\"7\"></td>\n",
    "<td style=\"text-align:center\">Github :<a href=\"https://github.com/These-SCAI2023/EXPE39_DHENS-2023_13092023\">https://github.com/These-SCAI2023</a>  </td>\n",
    "</table>\n",
    "<table>\n",
    "    <td style=\"text-align:center\"><img src=\"../DATA/images/ObTIC-logo-blue.png\" width=\"100\" height=\"50\"></td>\n",
    "    <td style=\"text-align:center\"><img src=\"../DATA/images/scai_su_logo_large.png\" width=\"100\" height=\"50\"></td>\n",
    "    <td style=\"text-align:center\"><img src=\"../DATA/images/STIH.png\" width=\"100\" height=\"50\"></td>\n",
    "</table>\n",
    "    \n",
    "\n",
    "-----\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Citation**\n",
    "\n",
    "Koudoro-Parfait, C., Alrahabi, M., Dupont, Y., Lejeune, G., & Roe, G. (2023, juin 30). Mapping spatial named entities from noisy OCR output: Epimetheus from OCR to map. Digital Humanities 2023. Collaboration as Opportunity (DH2023), Graz, Austria. https://doi.org/10.5281/zenodo.8108050\n",
    "\n",
    "**Équipe autour du projet**\n",
    "\n",
    "* Caroline Koudoro-Parfait\n",
    "* Yoann Dupont\n",
    "* Marceau Hernandez\n",
    "* Johanna Cordova\n",
    "* Riccardo Barontini\n",
    "* Gaël Lejeune\n",
    "* Glenn Roe\n",
    "* Motasem Alrahabi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Angle expérimental : Impact de la reconnaissance optique de caractères (OCR) sur la reconnaissance d'entités nommées (REN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Les données d'entrée : difficultés pour l'OCR \n",
    " <h3>European Literary Text Collection (ELTeC)</h3>\n",
    "    <td style=\"text-align:center\">Site web :<a href=\"https://www.distant-reading.net/eltec/\">https://www.distant-reading.net/eltec/</a>  </td>\n",
    "    <br>\n",
    "    <td style=\"text-align:center\">Zenodo :<a href=\"https://zenodo.org/communities/eltec/?page=1&size=20\">https://zenodo.org/communities/eltec/?page=1&size=20</a>  </td>\n",
    "    \n",
    "<table>\n",
    "    <tr> \n",
    "    <td style=\"text-align:center\"><img src=\"../DATA/images/CARRAUD_IMAGES.png\" width=\"300\" height=\"150\"></td>\n",
    "    <td style=\"text-align:center\"><img src=\"../DATA/images/BALZAC_PDF.png\" width=\"300\" height=\"150\"></td>\n",
    "    </tr> \n",
    "     <tr> \n",
    "    <td style=\"text-align:center\"><h3>Illustrations</h3></td>\n",
    "         <td style=\"text-align:center\"><h3>double colonnes</h3></td>\n",
    "    </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Textes sur deux colonnes\n",
    "\n",
    "<h3>Vert : colonne de droite</h3>\n",
    "<br>\n",
    "<img src=\"../DATA/images/balzac.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## L'épineuse question de la correction automatique\n",
    "\n",
    "Évaluation de la librairie **jamspell** : https://github.com/bakwc/JamSpell\n",
    "\n",
    "\n",
    "<img src=\"../DATA/images/correction_automatique.png\">\n",
    "\n",
    " - **MOBC** - mal océrisées bien corrigées ; \n",
    " - **MOMC** - mal océrisées mal corrigées ; \n",
    " - **MOI** - mal océrisées ignorées ; \n",
    " - **BOIC** - bien océrisées indûment corrigées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Élaboration de la chaine de traitement Epiméthée : de l'OCR à la carte\n",
    "\n",
    "*Épiméthée est une chaîne de traitement qui permet de bout-en-bout d'extraire les entités nommées (EN) de lieu dans un texte (numérisé ou non), puis de récupérer leurs coordonnées géographiques (Geopy) afin de les cartographier. Pour aider l'utilisateur à mieux filtrer la sortie qui peut être bruitée nous proposons une étape de clusterisation des EN.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. La chaine de traitement\n",
    "\n",
    "### 2. Les différentes étapes de développement\n",
    "\n",
    "### 3. Interface d'Épiméthée\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Schéma de la chaine de traitement\n",
    "\n",
    "<img src=\"../DATA/images/epimethee_workflow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Avant de commencer procéder à l'installation : </h3>\n",
    "<br>\n",
    "    <td style=\"text-align:center\">spaCy :<a href=\"https://spacy.io/\">https://spacy.io/</a>  </td>\n",
    "    <br>\n",
    "    <td style=\"text-align:center\">geopy :<a href=\"https://geopy.readthedocs.io/en/stable/\">https://geopy.readthedocs.io/en/stable/</a>  </td>\n",
    "    <br>\n",
    "    <td style=\"text-align:center\">seaborn :<a href=\"https://seaborn.pydata.org/\">https://seaborn.pydata.org/</a>  </td>\n",
    "    <br>\n",
    "    <td style=\"text-align:center\">sklearn :<a href=\"https://scikit-learn.org/stable/install.html\">https://scikit-learn.org/stable/install.html</a>  </td>\n",
    "    <br>\n",
    "    <td style=\"text-align:center\">pandas :<a href=\"https://pandas.pydata.org/\">https://pandas.pydata.org/</a>  </td>\n",
    "    <br>\n",
    "    <td style=\"text-align:center\">numpy :<a href=\"https://numpy.org/\">https://numpy.org/</a>  </td>\n",
    "    <br>\n",
    "    <td style=\"text-align:center\">numpy :<a href=\"https://pypi.org/project/matplotlib/\">https://pypi.org/project/matplotlib/</a>  </td>\n",
    "    <br>\n",
    "    <td style=\"text-align:center\">numpy :<a href=\"https://pypi.org/project/matplotlib-venn/\">https://pypi.org/project/matplotlib-venn/</a>  </td>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#La fonction qui sert à lire les textes pour tous le notebook. \n",
    "#Attention si vous souhaitez lire un autre format de fichier :csv, json, etc., elle n'est pas adéquate\n",
    "def read_text(path_corpora):\n",
    "        with open (path_corpora, \"r\", encoding=\"utf-8\") as fichier:\n",
    "            txt=fichier.read()\n",
    "        return txt\n",
    "    \n",
    "\n",
    "path_corpus= \"../DATA/ELTeCFRA/ADAM/*/*.txt\"\n",
    "for path in glob.glob(path_corpus):\n",
    "    nom_fichier=path.split(\"/\")[-1]\n",
    "    texte=read_text(path)\n",
    "    print(f\"_________Titre du texte lu : {nom_fichier}__________\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Le texte lu :\\n\\n\\n {texte[:250]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reconnaissance d'entités nommées (REN) sur des données bruitées avec spaCy\n",
    "\n",
    "https://spacy.io/\n",
    "\n",
    "SpaCy est un outil de traitement automatique des langues qui peut également être utilisé pour la REN.\n",
    "\n",
    "Il permet de retrouver les EN accompagnées d'un libellé, ainsi que la position du caractère de début et de fin du terme identifié. \n",
    "\n",
    "Nom de lieu : LOC ; \n",
    "Nom de la personne : PERS ; \n",
    "Organisations : ORG ; ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# chargement du modèle de langue de spaCy avec lequel vous souhaitez travailler\n",
    "#Vous pouvez changer le modèle de langue : 0=\"fr_core_news_lg\", 1=\"fr_core_news_md\", 2=\"fr_core_news_sm\"\n",
    "\n",
    "modele_langfr=[\"fr_core_news_lg\",\"fr_core_news_md\",\"fr_core_news_sm\"]\n",
    "modele=modele_langfr[0] \n",
    "nlp = spacy.load(modele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## displaCy :  Visualisation des sorties bruitées de REN\n",
    "https://demos.explosion.ai/displacy-ent\n",
    "\n",
    "**Vous devez arrêter le programme pour pouvoir passer à l'affichage suivant.**\n",
    "\n",
    "\n",
    "<h3>--> Displacy permet d'afficher les EN récupérées par spaCY mais pas de les stocker dans un format de données réutilisables</h3>\n",
    "\n",
    "\n",
    "Légende :\n",
    " - \"REF\": texte de reference \n",
    "\n",
    " - \"Kraken\": Kraken modèle de base\n",
    "\n",
    " - \"Tesseract\": Tesseract modèle adapté à la langue du corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arborescence de fichier\n",
    "<img src=\"../DATA/images/arborescence.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Importer le visualiseur Displacy de Spacy\n",
    "from spacy import displacy\n",
    "\n",
    "#cette boucle permet la lecture, l'analyse et l'affichage pour les différentes versions du texte\n",
    "for path in glob.glob(path_corpus):\n",
    "    version=path.split(\"/\")[3]\n",
    "    name_version=version.split(\"_\")[-1]\n",
    "    text=read_text(path)\n",
    "    \n",
    "    if \"REF\" in path:\n",
    "        print(\"Reference text output :\" )\n",
    "        doc1 =  nlp(text[:2000])\n",
    "        \n",
    "    if \"Kraken\" in path:\n",
    "        doc2 =  nlp(text[:2000])\n",
    "        \n",
    "    if \"Tesseract\" in path:\n",
    "        doc3 =  nlp(text[:2000])\n",
    "    \n",
    "#Afficher les résultats pour la REN avec le visualiser Displacy\n",
    "disp=displacy.serve(doc1,style=\"ent\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Kraken output : \")\n",
    "displacy.serve(doc2,style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Tesseractfr text output :\" )\n",
    "displacy.serve(doc3,style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Récupérer les résultats de REN dans plusieurs formats :\n",
    "\n",
    " -  sous forme de tableau avec pandas\n",
    " - sous forme de tableau csv\n",
    " - au format json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#stockage des données dans un format json pour une réutilisation des données\n",
    "def stocker(chemin, contenu):\n",
    "    w =open(chemin, \"w\")\n",
    "    w.write(json.dumps(contenu , indent = 2))\n",
    "    w.close()\n",
    "    # print(chemin)\n",
    "    \n",
    "    return chemin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for path in glob.glob(path_corpus):\n",
    "\n",
    "    dict_ent={}\n",
    "    liste_ent=[]\n",
    "    liste_label=[]\n",
    "    liste_contxt_droit=[]\n",
    "    liste_contxt_gauche=[]\n",
    "    auteur=path.split(\"/\")[3]\n",
    "    version=path.split(\"/\")[-2]\n",
    "    version=version.split(\"_\")[-1]\n",
    "    print(\"------------\",auteur,version)\n",
    "\n",
    "    \n",
    "    text=read_text(path)\n",
    "    #print(f\"          \\n\\n {text}\")\n",
    "    \n",
    "    doc = nlp(text)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_==\"LOC\":### Retrieve spacial entities only\n",
    "            liste_ent.append(ent.text)\n",
    "            liste_label.append(ent.label_)\n",
    "            liste_contxt_gauche.append(text[ent.start_char-15:ent.start_char])\n",
    "\n",
    "            liste_contxt_droit.append(text[ent.end_char:ent.end_char+15])\n",
    "        \n",
    "    dict_ent[\"Entite\"]=liste_ent\n",
    "    dict_ent[\"Label\"]=liste_label\n",
    "    dict_ent[\"liste_contxt_gauche\"]=liste_contxt_gauche\n",
    "    dict_ent[\"liste_contxt_droit\"]=liste_contxt_droit\n",
    "    \n",
    "    data_tab = pd.DataFrame(dict_ent)\n",
    "    \n",
    "    display(data_tab)  \n",
    "    data_tab.to_csv(f\"../DATA/ELTeCfra{version}.csv\", sep=\";\")\n",
    "    \n",
    "    \n",
    "    #stockage des EN concatenées pour la phase d'evaluation. \n",
    "    # La concatenation des En permets une meilleure comparaison ?\n",
    "    list_concat=[]\n",
    "    for i in liste_ent:\n",
    "        \n",
    "        \n",
    "        i_replace = i.replace(\" \", \"\")\n",
    "        \n",
    "        list_concat.append(i_replace)\n",
    "    stocker(\"%s-%s-concat.json\"%(path,modele),list_concat) \n",
    "        \n",
    "       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation de la REN sur des données bruitées : limite des intersections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn2_circles\n",
    "\n",
    "#Lire les fichiers json \n",
    "def read_json (chemin):\n",
    "    with open(chemin) as json_data: \n",
    "        data =json.load(json_data)\n",
    "    return data\n",
    "\n",
    "#calcul des diagrammes de Venn\n",
    "def diagramme_venn(liste_en_pp, liste_en_ocr,ocr):\n",
    "    font2 = {'size': 25} # use for labels\n",
    "    plt.rc('font', **font2) # sets the default font \n",
    "    plt.rcParams['text.color'] = 'black' # changes default text colour\n",
    "    venn2([set(liste_en_pp), set(liste_en_ocr)],set_labels = ('EN Réf', 'EN %s'%ocr),set_colors=(\"darkgrey\", \"darkblue\"),alpha=0.5)\n",
    "\n",
    "    venn2_circles(subsets=(set(liste_en_pp), set(liste_en_ocr)),linestyle=\"dotted\", linewidth=1) \n",
    "    print(f\"Analyse de la version {ocr}\")     \n",
    "\n",
    "#stockage des diagrammes de Venn au format png\n",
    "def stocker_png(nom_fichier):\n",
    "    plt.gcf().set_size_inches(12, 7)\n",
    "    plt.subplots_adjust(left=0.03, right=0.8, top=0.9, bottom=0.1)\n",
    "    plt.savefig(nom_fichier, dpi=300)\n",
    "    plt.gcf()\n",
    "   # plt.clf()\n",
    "    return nom_fichier\n",
    " \n",
    "    \n",
    "path_data= \"../DATA/ELTeCFRA2/*/*-concat.json\"\n",
    "#Stockage des données dans des structures listes pour ensuite les stocker dans une structure dictionnaire \n",
    "#utilisable avec Pandas\n",
    "dict_global={}\n",
    "liste_ocr=[]\n",
    "liste_entit=[]\n",
    "liste_vers=[]\n",
    "\n",
    "for path in glob.glob(path_data):\n",
    "    data=read_json(path)\n",
    "    \n",
    "    vers=path.split(\"/\")[3]\n",
    "    vers=vers.split(\"_\")[-1]\n",
    "    \n",
    "    if vers!=\"REF\":\n",
    "        liste_ocr.append(vers)\n",
    "      \n",
    "\n",
    "    for entity in data:\n",
    "            liste_entit.append(entity)\n",
    "            liste_vers.append(vers)\n",
    "            \n",
    "    \n",
    "        \n",
    "dict_global[\"Entite\"]=liste_entit\n",
    "dict_global[\"version\"]=liste_vers\n",
    "data_tab = pd.DataFrame(dict_global)\n",
    "\n",
    "\n",
    "dic_inter={}\n",
    "for name_ocr in liste_ocr:\n",
    "    \n",
    "    \n",
    "\n",
    "    data_ref=data_tab.query(\"version=='REF'\" )\n",
    "    data_ocr=data_tab.query(\"version== @name_ocr\" )\n",
    "   \n",
    "    diagramme_venn(data_ref.Entite,data_ocr.Entite,name_ocr)\n",
    "    \n",
    "    dic_inter[name_ocr]={}\n",
    "    intersection=set(data_ref.Entite)&set(data_ocr.Entite)\n",
    "    ens_ocr=set(data_ocr.Entite)-intersection\n",
    "    ens_ref=set(data_ref.Entite)-intersection\n",
    "    dic_inter[name_ocr][\"intersection\"]=list(intersection)\n",
    "    dic_inter[name_ocr][\"Contamine\"]=list(ens_ocr)\n",
    "    dic_inter[name_ocr][\"Ref\"]=list(ens_ref)\n",
    "  \n",
    "    \n",
    "    stocker_png(\"../DATA/INTERSECTION_GLOBALES/%s_%s_concat-intersection.png\"%(name_ocr,modele))  \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../DATA/images/en_contaminees.png\" width=\"800\" height=\"150\">\n",
    "\n",
    "<h4>Différentes formes contaminées d'une même EN</h4>\n",
    "<h4>ELTeC collection française</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../DATA/images/type_inpact_ocrner.png\" width=\"800\" height=\"150\">\n",
    "\n",
    "<h4>ELTeC collection française, portugaise et Anglaise</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../DATA/images/annotation_manuelle.png\" width=\"800\" height=\"150\">\n",
    "\n",
    "<h3>Annotation manuelle des VP et FP, manque-t-il vraiment des Entités ?</h3>\n",
    "<h4>Daudet, \"Le petit chose\", 1868.</h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lier des formes contaminées entre elles avec des mesures de similarités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import DistanceMetric\n",
    "#from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "liste2=[]\n",
    "liste3=[]\n",
    "liste_en1=[]\n",
    "liste_en22=[]\n",
    "liste_en2=[]\n",
    "liste_en3=[]\n",
    "dict_ent1={}\n",
    "dict_ent2={}\n",
    "\n",
    "#Réutilisation du dictionnaire \"dic_inter\"\n",
    "\n",
    "for OCR, values in dic_inter.items():\n",
    "    print(\"*****************************\",OCR)\n",
    "    for statut, EN in values.items():\n",
    "        if statut == \"intersection\":\n",
    "            liste1=EN\n",
    "            \n",
    "        if statut == \"Contamine\":\n",
    "            liste2=EN\n",
    "        if statut == \"Ref\":\n",
    "            liste3=EN\n",
    "            \n",
    "            for en1 in liste1:\n",
    "                for en2 in liste2:\n",
    "                    if len(en1) >2 and len(en2)>2:\n",
    "                        V = CountVectorizer(ngram_range=(1,4),analyzer='char')\n",
    "                        X = V.fit_transform([en1, en2]).toarray()\n",
    "                        distance_tab1=sklearn.metrics.pairwise.cosine_distances(X)                  \n",
    "                        distance=float(distance_tab1[0][1])\n",
    "                        if distance<0.35:\n",
    "                            #print(\"EN1 : \",en1,\"--\",\"EN2 : \", en2,\" :\", distance) \n",
    "                            liste_en1.append(en1)\n",
    "                            liste_en2.append(en2)\n",
    "                            \n",
    "            for en2 in liste2:\n",
    "                for en3 in liste3:\n",
    "                    if len(en2) >2 and len(en3)>2:\n",
    "                        V = CountVectorizer(ngram_range=(1,4),analyzer='char')\n",
    "                        X = V.fit_transform([en2, en3]).toarray()\n",
    "                        distance_tab1=sklearn.metrics.pairwise.cosine_distances(X)                  \n",
    "                        distance=float(distance_tab1[0][1])\n",
    "\n",
    "                        if distance<0.35:\n",
    "                            liste_en22.append(en2)\n",
    "                            liste_en3.append(en3)\n",
    "                            \n",
    "\n",
    "    dict_ent1[\"intersection\"]=liste_en1\n",
    "    dict_ent1[\"Contamine\"]=liste_en2\n",
    "    \n",
    "    dict_ent2[\"Réf.\"]=liste_en3\n",
    "    dict_ent2[\"Contamine\"]=liste_en22\n",
    "    \n",
    "\n",
    "    data_tab1 = pd.DataFrame(dict_ent1)\n",
    "    data_tab1=data_tab1.sort_values(by = 'Contamine')\n",
    "    display(data_tab1) \n",
    "    \n",
    "    data_tab2 = pd.DataFrame(dict_ent2)\n",
    "    data_tab2=data_tab2.sort_values(by = 'Contamine')\n",
    "    display(data_tab2) \n",
    "               \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering avec un algortihme d'affinité de propagation pour retrouver les variantes d'un même terme\n",
    "\n",
    " - **Pas besoin** de déterminer le **nombre de clusters** attendus \n",
    " - Les clusters sont calculés avec la métrique **cosinus** \n",
    " - en **bigramme** et **trigramme de caractères**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from collections import OrderedDict\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def nomfichier(chemin):\n",
    "    nomfich= chemin.split(\"/\")[-1]\n",
    "    nomfich= nomfich.split(\".\")\n",
    "    nomfich= (\"_\").join([nomfich[0],nomfich[1]])\n",
    "    return nomfich\n",
    "    \n",
    "\n",
    "def stocker(chemin,contenu):                                                   \n",
    "    w=open(chemin,\"w\")                                                         \n",
    "    w.write(json.dumps(contenu, indent=2))                                     \n",
    "    w.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "path_copora = \"../DATA/ELTeCFRA/DAUDET/*\"\n",
    "\n",
    "for subcorpus in glob.glob(path_copora):\n",
    "    print(\"SUBCORPUS***\",subcorpus)\n",
    "    liste_nom_fichier =[]\n",
    "    for path in glob.glob(\"%s/*lg-concat.json\"%subcorpus):\n",
    "#        print(\"PATH*****\",path)\n",
    "        \n",
    "        nom_fichier = nomfichier(path)\n",
    "        print(nom_fichier)\n",
    "        liste=read_json(path)  \n",
    "    #print(liste)        \n",
    "#### FREQUENCE ########\n",
    "        \n",
    "        dic_mots={}\n",
    "        i=0\n",
    "    \n",
    "        \n",
    "        for mot in liste: \n",
    "            \n",
    "            if mot not in dic_mots:\n",
    "                dic_mots[mot] = 1\n",
    "            else:\n",
    "                dic_mots[mot] += 1\n",
    "                #dic_langues[langue][mot] = dic_langues[langue][mot] + 1\n",
    "        \n",
    "        i += 1\n",
    "        #    print(dic_mots)\n",
    "        new_d = OrderedDict(sorted(dic_mots.items(), key=lambda t: t[0]))\n",
    "        \n",
    "#        print(new_d)\n",
    "        freq=len(dic_mots.keys())      \n",
    "#### VECTORISATION\n",
    "\n",
    "        Set_00 = set(liste)\n",
    "        Liste_00 = list(Set_00)\n",
    "        dic_output = {}\n",
    "        liste_words=[]\n",
    "        matrice=[]\n",
    "        \n",
    "        for l in Liste_00:\n",
    "            if len(l)!=1:\n",
    "                liste_words.append(l)\n",
    "\n",
    "        try:\n",
    "            words = np.asarray(liste_words) #So that indexing with a list will work\n",
    "            for w in words:\n",
    "                liste_vecteur=[]\n",
    "            \n",
    "                    \n",
    "                for w2 in words:\n",
    "                \n",
    "                        V = CountVectorizer(ngram_range=(2,3), analyzer='char', lowercase=False) #le paramètre ngram_range est paramétrable)\n",
    "                        X = V.fit_transform([w,w2]).toarray()\n",
    "                        distance_tab1=sklearn.metrics.pairwise.cosine_distances(X) # Distance avec cosinus            \n",
    "                        liste_vecteur.append(distance_tab1[0][1])\n",
    "                    \n",
    "            #    print(liste_vecteur)\n",
    "            \n",
    "            \n",
    "            #  \n",
    "                matrice.append(liste_vecteur)\n",
    "            matrice_def=-1*np.array(matrice)\n",
    "            #print(matrice)\n",
    "            \n",
    "    ##### CLUSTER\n",
    "                  \n",
    "            affprop = AffinityPropagation(affinity=\"precomputed\", damping= 0.5, random_state = None) \n",
    "    \n",
    "            affprop.fit(matrice_def)\n",
    "            for cluster_id in np.unique(affprop.labels_):\n",
    "                exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "                cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "                cluster_str = \", \".join(cluster)\n",
    "                cluster_list = cluster_str.split(\", \")\n",
    "            #                print(\" - *%s:* %s\" % (exemplar, cluster_str))                \n",
    "                Id = \"ID \"+str(i)\n",
    "                for cle, dic in new_d.items(): \n",
    "                    if cle == exemplar:\n",
    "                        dic_output[Id] ={}\n",
    "                        dic_output[Id][\"Centroïde\"] = exemplar\n",
    "                        dic_output[Id][\"Freq. centroide\"] = dic\n",
    "                        dic_output[Id][\"Termes\"] = cluster_list\n",
    "                \n",
    "                i=i+1\n",
    "            #print(dic_output)\n",
    "            data_cluster = pd.DataFrame(dic_output)\n",
    "            display(data_cluster) \n",
    "##STOCKER les dictionnaires au format json\n",
    "            #stocker(\"%s/%s_cluster-consinus-2-3-clean.json\"%(subcorpus,nom_fichier),dic_output)\n",
    "            # stocker(\"%s/%s_cluster-consinus-3-4-clean.json\"%(subcorpus,nom_fichier),dic_output)\n",
    "\n",
    "        except :        \n",
    "            print(\"**********Non OK***********\", path)\n",
    "\n",
    "    \n",
    "            liste_nom_fichier.append(path)\n",
    "           # stocker(\"%s/fichier_non_cluster.json\"%subcorpus, liste_nom_fichier)\n",
    "            \n",
    "            continue \n",
    "#   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualiser les clusters avec seaborn et matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "def nomfichier(chemin):\n",
    "    nomfich= chemin.split(\"/\")[-1]\n",
    "    nomfich= nomfich.split(\".\")\n",
    "    nomfich= (\"_\").join([nomfich[0]])\n",
    "    return nomfich\n",
    "\n",
    "centro=[]\n",
    "cluster=[]\n",
    "i=0\n",
    "\n",
    "#iy=1\n",
    "#centro=[]\n",
    "#cluster=[]\n",
    "liste_resultat=[]\n",
    "## En entrée le fichier csv annoté\n",
    "file_path=\"../DATA/SPACY_ANNOT/DAUDET/DAUDET_petit-chose_Kraken-base_txt_lg_spacy_cluster-cosinus-2-3_ANNOT.csv\"\n",
    "\n",
    "with open(file_path, newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=';', quotechar=' ')\n",
    "    next(spamreader)\n",
    "    \n",
    "    for row in spamreader:\n",
    "#        print(row)\n",
    "        liste_resultat.append(row)\n",
    "\n",
    "for liste in liste_resultat:\n",
    " \n",
    "\n",
    "    if liste[-1]==\"\":\n",
    "        centro.append([liste[1],liste[-2]])\n",
    "    else:\n",
    "        cluster.append([liste[1],liste[2],liste[-1]])\n",
    "            \n",
    "#print(cluster)\n",
    "for cl in cluster:\n",
    "#            for centr in centro:\n",
    "#                if cl[0]==centr[0]:\n",
    "#                    centr.append(cl)\n",
    " \n",
    "    if cl[0]==centro[i][0]:\n",
    "\n",
    "        centro[i].append(cl)\n",
    "#        print(\"Pareil\",cl[0],centro[i][0])\n",
    "\n",
    "    else:\n",
    "#        #        print(\"*************PAS Pareil\",cl[0],centro[i][0])\n",
    "        i=i+1\n",
    "        centro[i].append(cl)\n",
    "\n",
    "        \n",
    "tableau={}\n",
    "liste_label_centro=[]\n",
    "liste_label_cluster=[]\n",
    "liste_statut_centro=[]\n",
    "liste_statut_cluster=[]\n",
    "liste_ix=[]\n",
    "liste_resultat_dist2=[]\n",
    "liste_lower=[]\n",
    "ix=0\n",
    "icl=0\n",
    "\n",
    "liste_resultat_dist_low=[]\n",
    "\n",
    "for ce in centro:   \n",
    "#    ix+=10\n",
    "    ix=(ix+100)-ix%100\n",
    "    icl=0\n",
    "    for j in ce[2:]:\n",
    "        \n",
    "        if ce[0]==j[1]:\n",
    "            liste_ix.append(ix)\n",
    "        else:       \n",
    "            icl=icl+50\n",
    "            liste_ix.append(ix+icl)\n",
    "\n",
    "            \n",
    "        V = CountVectorizer(ngram_range=(2,3), analyzer='char')\n",
    "        #lower case =True par défaut\n",
    "        X = V.fit_transform([j[0],j[1]]).toarray()\n",
    "       \n",
    "        ### Distance Cosinus\n",
    "        distance_tab1=sklearn.metrics.pairwise.cosine_distances(X) \n",
    "        liste_resultat_dist2.append(distance_tab1[0][1])\n",
    " \n",
    "         \n",
    "        liste_statut_centro.append(ce[1])\n",
    "        liste_label_centro.append(ce[0])\n",
    "        liste_label_cluster.append(j[1])\n",
    "        liste_statut_cluster.append(j[2])\n",
    "    ix+=icl\n",
    "    #print(liste_ix)\n",
    "\n",
    "## CKP : Moyenne Distance, regularisation casse (pas utile selon les tests)\n",
    "\n",
    "       \n",
    "\n",
    "#liste_moy=[]\n",
    "#for jl in liste_lower:\n",
    "#    \n",
    "#    V_lower = CountVectorizer(ngram_range=(2,3), analyzer='char') \n",
    "#    Xlower = V_lower.fit_transform([jl[0],jl[1]]).toarray()\n",
    "#    distance_tab_low=sklearn.metrics.pairwise.cosine_distances(Xlower) \n",
    "#    liste_resultat_dist_low.append(distance_tab_low[0][1])\n",
    "\n",
    "    \n",
    "#moy_zip=zip(liste_resultat_dist2,liste_resultat_dist_low)  \n",
    "#for idd in moy_zip:\n",
    "#    if idd[0]!= idd[1]:\n",
    "#        print(idd[0],idd[1])\n",
    "    \n",
    "    \n",
    "coord_zip=zip(liste_resultat_dist2,liste_ix) \n",
    "#print (list(test_zip))\n",
    "coord=[]\n",
    "coord_cl=[]\n",
    "for ind in list(coord_zip):\n",
    "    coord_cl.append(ind)\n",
    "#    print(ind[0])\n",
    "    if ind[0]==0.0:\n",
    "        coord.append(ind)\n",
    "#        print(ind)\n",
    "\n",
    "liste_centr=[]\n",
    "for ce in centro: \n",
    "    liste_centr.append(ce[0]) \n",
    "\n",
    "liste_clus=[]\n",
    "for cl in cluster:\n",
    "    liste_clus.append(cl[1])\n",
    "#print(liste_clus)\n",
    "    \n",
    "groups={}\n",
    "\n",
    "for l,c in zip(liste_centr,coord): \n",
    "    groups[l]={}\n",
    "    groups[l]=c\n",
    "    \n",
    "groups_cl={}\n",
    "for lc,cc in zip(liste_clus,coord_cl): \n",
    "    groups_cl[lc]={}\n",
    "    groups_cl[lc]=cc\n",
    "\n",
    "\n",
    "tableau[\"label_centro\"]=liste_label_centro\n",
    "tableau[\"label_cluster\"]=liste_label_cluster\n",
    "tableau[\"Statut_cluster\"]=liste_statut_cluster\n",
    "tableau[\"Statut_centro\"]=liste_statut_centro\n",
    "tableau[\"Distances cosinus\"]=liste_resultat_dist2\n",
    "tableau[\"Centroïdes\"]=liste_ix\n",
    "        \n",
    "      \n",
    "data = pd.DataFrame(tableau)\n",
    "\n",
    "data.head()\n",
    "\n",
    "#y_min=0\n",
    "#y_max=1000\n",
    "\n",
    "facet2=sns.relplot(data=data, x=\"Distances cosinus\", y=\"Centroïdes\", hue=\"label_centro\", style=\"Statut_cluster\",s=40)\n",
    "facet2.set_yticklabels([])\n",
    "#Permet de changer la taille de la figure\n",
    "plt.gcf().set_size_inches(25, 25)\n",
    "\n",
    "for key, values in groups_cl.items():            \n",
    "        plt.annotate(key,values, horizontalalignment='right', verticalalignment='top',\n",
    "                     size=5, weight='bold')           \n",
    "\n",
    "        #La figure est stockée au bout du chemin suivant \n",
    "plt.savefig(\"../DATA/OUTPUT_CLUSTER/DAUDET_petit-chose_Kraken-base_lg_spacy_cluster-cosinus-2-3_ANNOT_ok.png\",dpi=300, bbox_inches=\"tight\")\n",
    "           \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../DATA/images/DAUDET_cluster2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinaison de plusieurs outils de REN pour filtrer les résultats\n",
    "\n",
    "Nous ajoutons le modèle Camenbert-ner aux modèles spaCy lg et sm.  \n",
    "    <td style=\"text-align:center\">Camenbert-ner :<a href=\"https://huggingface.co/Jean-Baptiste/camembert-ner\">https://huggingface.co/Jean-Baptiste/camembert-ner</a>  </td>  \n",
    "    les lignes de commande suivante procèdent à l'installation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!pip3 install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lire_fichier (chemin):\n",
    "    f = open(chemin , encoding = 'utf−8')\n",
    "    chaine = f.read ()\n",
    "    f.close ()\n",
    "    return chaine\n",
    "\n",
    "\n",
    "def titre_auteur(chemin):\n",
    "    nomfich= chemin.split(\"/\")[-1]\n",
    "    nomfich= nomfich.split(\".\")[0]\n",
    "    nomfich= nomfich.split(\"_\")\n",
    "    # nomfich= nomfich.upper()\n",
    "    # nomfich= (\"_\").join([nomfich[0]])\n",
    "    return nomfich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../DATA/ELTeCFRA/ADAM/ADAM_kraken/ADAM_Mon-village_Kraken-base_test.txt\n",
      "../DATA/ELTeCFRA/ADAM/ADAM_kraken/ADAM_Mon-village_Kraken-base_test.txt\n",
      "../DATA/ELTeCFRA/ADAM/ADAM_kraken/ADAM_Mon-village_Kraken-base_test.txt\n",
      "______________________________________________________\n",
      "résultats pour le modèle camenBert_ner ---> {'h IIIMON VILLAGE', 'Mlorlincourt', 'MO N VILLAGE'}\n",
      "______________________________________________________\n",
      "______________________________________________________\n",
      "résultats pour le modèle sm ---> {'verildc', 'Jeac-', 'Ca', 'vois-tu', 'Tournez', 'Adonc', 'Ouvrezl', 'VILLAGE', \"CG'\", 'Saint-Brunelle', 'Pierre', \"Ml'aimeras-tu\", 'Artaban', 'Mlaria', 'Faut', 'Jean-Claude', 'Assure', \"Ml'\", 'niere', 'Figurez', 'brin', 'Toinon', 'Iais', 'Fa-\\nvette', 'Savoyard', 'V1LLAGE', 'Crois-tu', '---- A', 'Cran', 'salete', 'sauta', 'ca', 'dragon', 'Feuil-', 'Norine', 'LL-.----------==-=x=-MON vILLAGE', 'Rose', 'va-t-en', 'Paris', 'Morlincourt', 'Inisc', 'lille', 'Tiens', 'g2yI'}\n",
      "______________________________________________________\n",
      "______________________________________________________\n",
      "résultats pour le modèle lg ---> {'parigole', 'Jeac-', 'Ca', 'Ouvrezl', 'Saint-Brunelle', \"CG'\", 'gne', \"Ml'\", 'carrieres', 'Ceran', 'LL-.----------==-=x=-MON', 'bu-', 'parigot de Therese', 'MO N VILLAGE', 'chemindes', 'Allons', 'Iais', 'M1ON V1LLAGE', 'A', 'fiertd', 'Fa-\\nvette', 'Savoyard', 'As-', 'Dicu', 'courtiseras', 'Feuil-', 'veillee', 'Norine', 'Morlincourt tot', 'Paris', 'Rose', 'emotionnee', 'LES', 'Morlincourt', 'peul lille', 'Inisc', 'MI', 'Brlee', 'rer'}\n",
      "______________________________________________________\n",
      "<class 'list'>\n",
      "______________________________________________________\n",
      "Intersection pour les ensembles  camenBert_ner  --  sm -->  set()\n",
      "______________________________________________________\n",
      "______________________________________________________\n",
      "Intersection pour les ensembles  camenBert_ner  --  lg -->  {'MO N VILLAGE'}\n",
      "______________________________________________________\n",
      "______________________________________________________\n",
      "Intersection pour les ensembles  sm  --  lg -->  {'Fa-\\nvette', 'Savoyard', 'Norine', 'Paris', 'Ca', 'Jeac-', 'Ouvrezl', 'Rose', 'Saint-Brunelle', \"CG'\", 'Morlincourt', 'Inisc', 'Iais', 'Feuil-', \"Ml'\"}\n",
      "______________________________________________________\n",
      "________________________Intersection entre les 3 modèles de langue__________________________________\n",
      "camenBert_ner -- sm -- lg  -->  set()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "\n",
    "\n",
    "##### Process text sample (from wikipedia)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "path_corpora = \"../DATA/ELTeCFRA/ADAM/ADAM_kraken/ADAM_Mon-village_Kraken-base_test.txt\"\n",
    "\n",
    "dico_entite={} ##stocker les sorties de camembert-ner\n",
    "\n",
    "\n",
    "nlp_camenBert = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "modele=[\"camenBert_ner\",\"sm\", \"lg\"]\n",
    "\n",
    "nlp.max_length = 3508822 \n",
    "for m in modele:\n",
    "    liste_entite=[]\n",
    "    for path in glob.glob(path_corpora):\n",
    "        # print(path)\n",
    "        titreauteur=titre_auteur(path)   \n",
    "        texte = lire_fichier(path)\n",
    "        if m == \"camenBert_ner\":\n",
    "            \n",
    "            text=nlp_camenBert(texte)\n",
    "            for entite in text:\n",
    "                # print(entite)\n",
    "                for key, value in entite.items():\n",
    "                    if value==\"LOC\":\n",
    "                        liste_entite.append(entite[\"word\"])\n",
    "            dico_entite[m]=set(liste_entite)\n",
    "        else:\n",
    "            nlp_spacy = spacy.load(\"fr_core_news_%s\"%m)\n",
    "            doc=nlp_spacy(texte)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ ==\"LOC\":\n",
    "                    liste_entite.append(ent.text)\n",
    "                    # print(m,\"--->\",ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "            dico_entite[m]=set(liste_entite)\n",
    "for k, value in dico_entite.items():\n",
    "    print(\"______________________________________________________\")\n",
    "    print(\"résultats pour le modèle\",k,\"--->\",value)\n",
    "    print(\"______________________________________________________\")\n",
    "    \n",
    "liste_key=[]\n",
    "liste_key=list(dico_entite.keys())\n",
    "print(type(liste_key))\n",
    "i=0\n",
    "j=0\n",
    "for i in range(len(liste_key)-1):\n",
    "    j=0\n",
    "    \n",
    "    while j < (len(liste_key)-1):\n",
    "        j+=1\n",
    "        \n",
    "        # print(\"______________________________________________________\")\n",
    "        # print(\"**********\",liste_key[i],dico_entite[liste_key[i]])\n",
    "        # print(\"----------\",liste_key[j],dico_entite[liste_key[j]])\n",
    "        # print(\"______________________________________________________\")\n",
    "        if liste_key[i]!=liste_key[j]:\n",
    "            print(\"______________________________________________________\")\n",
    "            print(\"Intersection pour les ensembles \",liste_key[i] ,\" -- \", liste_key[j], \"--> \",dico_entite[liste_key[i]].intersection(dico_entite[liste_key[j]]))\n",
    "            print(\"______________________________________________________\")\n",
    "            \n",
    "print(\"________________________Intersection entre les 3 modèles de langue__________________________________\")            \n",
    "i=0\n",
    "print(liste_key[i] ,\"--\", liste_key[i+1],\"--\", liste_key[i+2],\" --> \",dico_entite[liste_key[i]].intersection(dico_entite[liste_key[i+1]].intersection(dico_entite[liste_key[i+2]])))    \n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Représenter les résultats sur une carte ?\n",
    "<h3>Cas d'usage</h3>\n",
    "<h3>Un collègue chercheur en Lettres a besoin d'un programme qui récupère sur 800 romans OCRisés :\n",
    "    <br>\n",
    " - les entités nommées, \n",
    "    <br>\n",
    " - les contextes gauches et droits, \n",
    "    <br>\n",
    " - différentes métadonnées.\n",
    "    <br>\n",
    "    Il veut récupérer toutes les EN, mais souhaite que les EN LOC soient **géolocalisées**.</h3>\n",
    "\n",
    "Problèmes 1 : Le géocodeur ne **trouve pas toujours** de géolocalisation pour une EN.\n",
    "\n",
    "Commentaires : \n",
    " - le géocodeur sert de filtre : il ne s'agit pas d'une EN, c'est du **bruit**, \n",
    " - il n'y en a **pas beaucoup** par fichier. \n",
    "      - **Solution : Complétion à la main dans un fichier csv.**\n",
    " - il y en a beaucoup trop il faut améliorer le programme de geocoding ? \n",
    " \n",
    "Problèmes 2 : Le géocodeur **trouve** la géolocalisation pour une EN LOC, qui en fait est du bruit :\n",
    "- **c'est vrai il y en a, mais c'est toujours moins long que de corriger tous le texte à la main ?** \n",
    "    - **Solution** : changement de label et supression de la localisation à la main dans un fichier csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "def lire_fichier (chemin):\n",
    "    f = open(chemin , encoding = 'utf−8')\n",
    "    chaine = f.read ()\n",
    "    f.close ()\n",
    "    return chaine\n",
    "\n",
    "\n",
    "def titre_auteur(chemin):\n",
    "    nomfich= chemin.split(\"/\")[-1]\n",
    "    nomfich= nomfich.split(\".\")[0]\n",
    "    nomfich= nomfich.split(\"_\")\n",
    "    # nomfich= nomfich.upper()\n",
    "    # nomfich= (\"_\").join([nomfich[0]])\n",
    "    return nomfich\n",
    "\n",
    "## MAIN\n",
    "y=0\n",
    "geo_flag=0\n",
    "\n",
    "path_corpora = \"../DATA/ELTeCFRA/DAUDET/DAUDET_TesseractFra-PNG/*.txt\"\n",
    "\n",
    "list_resultats=[]\n",
    "liste_res=[]\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "nlp.max_length = 3508822 \n",
    "for path in glob.glob(path_corpora):\n",
    "    print(path)\n",
    "    titreauteur=titre_auteur(path)   \n",
    "    texte = lire_fichier(path)\n",
    "    text=nlp(texte)       \n",
    "    liste_tok=[]\n",
    "    for token in text:\n",
    "        liste_tok.append(token.text)\n",
    "    \n",
    "    for ent in text.ents:\n",
    "        list_resultats.append([titreauteur[0],titreauteur[1],len(liste_tok),ent.text, ent.start_char, ent.end_char, ent.label_,texte[ent.start_char-200:ent.start_char],texte[ent.end_char:ent.end_char+200]])\n",
    "        \n",
    "\n",
    "for i in list_resultats:\n",
    "\n",
    "    start_caract = i[4]\n",
    "    end_caract =i[5]\n",
    "    # print(start_caract,end_caract)\n",
    "    if i[-3]==\"LOC\":\n",
    "        # print(i)\n",
    "        ## Geolocalisation avec Nominatim\n",
    "        try:\n",
    "            res = i[3].replace(\"\\n\\n\", \"\")\n",
    "            geolocator = Nominatim(user_agent=\"htpss\")\n",
    "            location = geolocator.geocode(i[3],timeout=30)\n",
    "            liste_res.append([i[0],i[1],\"année \",i[2],i[3],i[-3],location.latitude ,location.longitude,i[4],i[5],i[-2],i[-1]])\n",
    "            print(\"OK\",i[3],i[-3])\n",
    "        except:\n",
    "            ## On gère les exceptions : il y a une EN labelisée LOC mais pas de geoloc.\n",
    "                print(\"!!! Pas de localisation pour le LOC : \", i[3])\n",
    "                liste_res.append([i[0],i[1],\" année\",i[2],i[3],i[-3],\"None\",\"None\",i[4],i[5],i[-2],i[-1]])\n",
    "                continue\n",
    "    else:\n",
    "        liste_res.append([i[0],i[1],\"année \",i[2],i[3],i[-3],\"None \",\"None\",i[4],i[5],i[-2],i[-1]])\n",
    "        #print(\"Entité label autre :\",i[3],\",\",i[-3])\n",
    "\n",
    "## Stockage au format .csv  \n",
    "## Utilisation dans GoogleMymap : paramétrage du delimiter avec une \",\" à la place du \";\"\n",
    "with open(\"../DATA/OUTPUT_CSV/toto.csv\", 'w', newline='', encoding = 'utf−8') as csvfile:\n",
    "            spamwriter = csv.writer(csvfile, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            spamwriter.writerow([\"auteur\"]+[\"titre\"]+[\"annee\"]+[\"nb_tokens_texte\"]+[\"Entite\"] + [\"Label\"]+[\"latitude\"]+[\"longitude\"]+[\"caractere_debut\"]+[\"caractere_fin\"]+[\"contexte_gauche\"]+[\"contexte_droit\"])  \n",
    "            for res in liste_res:\n",
    "                spamwriter.writerow(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Des programmes au logiciel : rendre le code robuste et produire une Interface\n",
    "\n",
    "Le prototype est disponible sur __Pandore__ la toolbox proposée par __ObTIC__.\n",
    "https://pandore-toolbox.isir.upmc.fr/ocr_map\n",
    "<table>\n",
    "    <td style=\"text-align:center\"><img src=\"../DATA/images/github.png\" width=\"15\" height=\"7\"></td>\n",
    "<td style=\"text-align:center\">Github :<a href=\"https://github.com/obtic-sorbonne/Toolbox-site/tree/nermap\">https://github.com/obtic-sorbonne/Toolbox-site/tree/nermap</a>  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"../DATA/images/prototype.png\" width=\"500\">\n",
    "\n",
    "Toutes les configuration ne fonctionnent pas encore. \n",
    "\n",
    "**La version en ligne sur le site de Pandore rencontre de nombreux problème et n'ayant pas accès au serveur nous ne pouvons pas la maintenir. Nous préparons un container docker bientôt téléchargeable pour utiliser Epiméthée localement sur votre machine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../DATA/images/EPIMETHEE_10-10-2023.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../DATA/images/EPIMETHEE_10-10-2023_MORLINCOURT.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filtrage des clusters\n",
    "<table>\n",
    "    <tr> \n",
    "    <td style=\"text-align:center\"><img src=\"../DATA/images/ADAM_KRAKEN_CLUSTER_AVANT00.png\" width=\"600\" height=\"150\"></td>\n",
    "    <td style=\"text-align:center\"><img src=\"../DATA/images/ADAM_KRAKEN_CLUSTER_APRES00.png\" width=\"600\" height=\"150\"></td>\n",
    "    </tr> \n",
    "     <tr> \n",
    "    <td style=\"text-align:center\"><h3>Avant filtage</h3></td>\n",
    "         <td style=\"text-align:center\"><h3>Après filtrage</h3></td>\n",
    "    </tr> \n",
    "</table>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
